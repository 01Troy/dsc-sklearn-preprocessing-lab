{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Preprocessing with scikit-learn - Cumulative Lab\n", "\n", "## Introduction\n", "In this cumulative lab, you'll practice applying various preprocessing techniques with scikit-learn (`sklearn`) to the Ames Housing dataset in order to prepare the data for predictive modeling. The main emphasis here is on preprocessing (not EDA or modeling theory), so we will skip over most of the visualization and metrics steps that you would take in an actual modeling process.\n", "\n", "## Objectives\n", "\n", "You will be able to:\n", "\n", "* Practice identifying which preprocessing technique to use\n", "* Practice filtering down to relevant columns\n", "* Practice applying `sklearn.impute` to fill in missing values\n", "* Practice applying `sklearn.preprocessing`:\n", "  * `LabelBinarizer` for converting binary categories to 0 and 1 within a single column\n", "  * `OneHotEncoder` for creating multiple \"dummy\" columns to represent multiple categories\n", "  * `PolynomialFeatures` for creating interaction terms\n", "  * `StandardScaler` for scaling data"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Your Task: Prepare the Ames Housing Dataset for Modeling\n", "\n", "![house in Ames](images/ames_house.jpg)\n", "\n", "<span>Photo by <a href=\"https://unsplash.com/@kjkempt17?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Kyle Kempt</a> on <a href=\"https://unsplash.com/s/photos/ames?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Unsplash</a></span>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Requirements\n", "\n", "#### 1. Drop Irrelevant Columns\n", "\n", "For the purposes of this lab, we will only be using a subset of all of the features present in the Ames Housing dataset. In this step you will drop all irrelevant columns.\n", "\n", "#### 2. Handle Missing Values\n", "\n", "Often for reasons outside of a data scientist's control, datasets are missing some values. In this step you will assess the presence of NaN values in our subset of data, and use `MissingIndicator` and `SimpleImputer` from the `sklearn.impute` submodule to handle any missing values.\n", "\n", "#### 3. Convert Categorical Features into Numbers\n", "\n", "A built-in assumption of the scikit-learn library is that all data being fed into a machine learning model is already in a numeric format, otherwise you will get a `ValueError` when you try to fit a model. In this step you will use a `LabelBinarizer` to replace data within individual non-numeric columns with 0s and 1s, and a `OneHotEncoder` to replace columns containing more than 2 categories with multiple \"dummy\" columns containing 0s and 1s.\n", "\n", "At this point, a scikit-learn model should be able to run without errors!\n", "\n", "#### 4. Add Interaction Terms\n", "\n", "This step gets into the feature engineering part of preprocessing. Does our model improve as we add interaction terms? In this step you will use a `PolynomialFeatures` transformer to augment the existing features of the dataset.\n", "\n", "#### 5. Scale Data\n", "\n", "Because we are using a model with regularization, it's important to scale the data so that coefficients are not artificially penalized based on the units of the original feature. In this step you will use a `StandardScaler` to standardize the units of your data.\n", "\n", "#### BONUS: Refactor into a Pipeline\n", "\n", "In a professional data science setting, this work would be accomplished mainly within a scikit-learn pipeline, not by repeatedly creating pandas `DataFrame`s, transforming them, and concatenating them together. In this step you will optionally practice refactoring your existing code into a pipeline (or you can just look at the solution branch)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Lab Setup\n", "\n", "### Getting the Data\n", "\n", "In the cell below, we import the `pandas` library, open the CSV containing the Ames Housing data as a pandas `DataFrame`, and inspect its contents."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "import pandas as pd\n", "df = pd.read_csv(\"data/ames.csv\")\n", "df"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "df.describe()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The prediction target for this analysis is the sale price of the home, so we separate the data into `X` and `y` accordingly:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "y = df[\"SalePrice\"]\n", "X = df.drop(\"SalePrice\", axis=1)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Next, we separate the data into a train set and a test set prior to performing any preprocessing steps:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "from sklearn.model_selection import train_test_split\n", "\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["(If you are working through this lab and you just want to start over with the original value for `X_train`, re-run the cell above.)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "print(f\"X_train is a DataFrame with {X_train.shape[0]} rows and {X_train.shape[1]} columns\")\n", "print(f\"y_train is a Series with {y_train.shape[0]} values\")\n", "\n", "# We always should have the same number of rows in X as values in y\n", "assert X_train.shape[0] == y_train.shape[0]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Fitting a Model\n", "\n", "For this lab we will be using an `ElasticNet` model from scikit-learn ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html)). You are welcome to read about the details of this model implementation at that link, but for the purposes of this lab, what you need to know is that this is a form of linear regression with *regularization* (meaning we will need to standardize the features).\n", "\n", "Right now, we have not done any preprocessing, so we expect that trying to fit a model will fail:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "from sklearn.linear_model import ElasticNet\n", "\n", "model = ElasticNet(random_state=1)\n", "model.fit(X_train, y_train)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["As you can see, we got `ValueError: could not convert string to float: 'RL'`.\n", "\n", "In order to fit a scikit-learn model, all values must be numeric, and the third column of our full dataset (`MSZoning`) contains values like `'RL'` and `'RH'`, which are strings. So this error was expected, but after some preprocessing, this model will work!"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1. Drop Irrelevant Columns\n", "\n", "For the purpose of this analysis, we'll only use the following columns, described by `relevant_columns`. You can find the full description of their values in the file `data/data_description.txt` included in this repository.\n", "\n", "In the cell below, reassign `X_train` so that it only contains the columns in `relevant_columns`.\n", "\n", "**Hint:** Even though we describe this as \"dropping\" irrelevant columns, it's easier if you invert the logic, so that we are only keeping relevant columns, rather than using the `.drop()` method. It is possible to use the `.drop()` method if you really want to, but first you would need to create a list of the column names that you don't want to keep."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Replace None with appropriate code\n", "\n", "# Declare relevant columns\n", "relevant_columns = [\n", "    'LotFrontage',  # Linear feet of street connected to property\n", "    'LotArea',      # Lot size in square feet\n", "    'Street',       # Type of road access to property\n", "    'OverallQual',  # Rates the overall material and finish of the house\n", "    'OverallCond',  # Rates the overall condition of the house\n", "    'YearBuilt',    # Original construction date\n", "    'YearRemodAdd', # Remodel date (same as construction date if no remodeling or additions)\n", "    'GrLivArea',    # Above grade (ground) living area square feet\n", "    'FullBath',     # Full bathrooms above grade\n", "    'BedroomAbvGr', # Bedrooms above grade (does NOT include basement bedrooms)\n", "    'TotRmsAbvGrd', # Total rooms above grade (does not include bathrooms)\n", "    'Fireplaces',   # Number of fireplaces\n", "    'FireplaceQu',  # Fireplace quality\n", "    'MoSold',       # Month Sold (MM)\n", "    'YrSold'        # Year Sold (YYYY)\n", "]\n", "\n", "# Reassign X_train so that it only contains relevant columns\n", "None\n", "\n", "# Visually inspect X_train\n", "X_train"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Check that the new shape is correct:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "\n", "# X_train should have the same number of rows as before\n", "assert X_train.shape[0] == 1095\n", "\n", "# Now X_train should only have as many columns as relevant_columns\n", "assert X_train.shape[1] == len(relevant_columns)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2. Handle Missing Values\n", "\n", "In the cell below, we check to see if there are any NaNs in the selected subset of data:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "X_train.isna().sum()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Ok, it looks like we have some NaNs in `LotFrontage` and `FireplaceQu`.\n", "\n", "Before we proceed to fill in those values, we need to ask: **do these NaNs actually represent** ***missing*** **values, or is there some real value/category being represented by NaN?**\n", "\n", "### Fireplace Quality\n", "\n", "To start with, let's look at `FireplaceQu`, which means \"Fireplace Quality\". Why might we have NaN fireplace quality?\n", "\n", "Well, some properties don't have fireplaces!\n", "\n", "Let's confirm this guess with a little more analysis.\n", "\n", "First, we know that there are 512 records with NaN fireplace quality. How many records are there with zero fireplaces?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "X_train[X_train[\"Fireplaces\"] == 0]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Ok, that's 512 rows, same as the number of NaN `FireplaceQu` records. To double-check, let's query for that combination of factors (zero fireplaces and `FireplaceQu` is NaN):"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "X_train[\n", "    (X_train[\"Fireplaces\"] == 0) &\n", "    (X_train[\"FireplaceQu\"].isna())\n", "]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Looks good, still 512 records. So, NaN fireplace quality is not actually information that is missing from our dataset, it is a genuine category which means \"fireplace quality is not applicable\". This interpretation aligns with what we see in `data/data_description.txt`:\n", "\n", "```\n", "...\n", "FireplaceQu: Fireplace quality\n", "\n", "       Ex\tExcellent - Exceptional Masonry Fireplace\n", "       Gd\tGood - Masonry Fireplace in main level\n", "       TA\tAverage - Prefabricated Fireplace in main living area or Masonry Fireplace in basement\n", "       Fa\tFair - Prefabricated Fireplace in basement\n", "       Po\tPoor - Ben Franklin Stove\n", "       NA\tNo Fireplace\n", "...\n", "```\n", "\n", "Eventually we will still need to perform some preprocessing to prepare the `FireplaceQu` column for modeling (because models require numeric inputs), but we don't need to worry about filling in missing values."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Lot Frontage\n", "\n", "Now let's look at `LotFrontage` \u2014 it's possible that NaN is also a genuine category here, and it's possible that it's just missing data instead. Let's apply some domain understanding to understand whether it's possible that lot frontage can be N/A just like fireplace quality can be N/A.\n", "\n", "Lot frontage is defined as the \"Linear feet of street connected to property\", i.e. how much of the property runs directly along a road. The amount of frontage required for a property depends on its zoning. Let's look at the zoning of all records with NaN for `LotFrontage`:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "df[df[\"LotFrontage\"].isna()][\"MSZoning\"].value_counts()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["So, we have RL (residential low density), RM (residential medium density), FV (floating village residential), and RH (residential high density). Looking at the building codes from the City of Ames, it appears that all of these zones require at least 24 feet of frontage.\n", "\n", "Nevertheless, we can't assume that all properties have frontage just because the zoning regulations require it. Maybe these properties predate the regulations, or they received some kind of variance permitting them to get around the requirement. **It's still not as clear here as it was with the fireplaces whether this is a genuine \"not applicable\" kind of NaN or a \"missing information\" kind of NaN.**\n", "\n", "In a case like this, we can take a double approach:\n", "\n", "1. Make a new column in the dataset that simply represents whether `LotFrontage` was originally NaN\n", "2. Fill in the NaN values of `LotFrontage` with median frontage in preparation for modeling"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Missing Indicator for `LotFrontage`\n", "\n", "First, we import `sklearn.impute.MissingIndicator` ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.impute.MissingIndicator.html)). The goal of using a `MissingIndicator` is creating a new column to represent which values were NaN (or some other \"missing\" value) in the original dataset, in case NaN ends up being a meaningful indicator rather than a random missing bit of data.\n", "\n", "A `MissingIndicator` is a scikit-learn transformer, meaning that we will use the standard steps for any scikit-learn transformer:\n", "\n", "1. Identify data to be transformed (typically not every column is passed to every transformer)\n", "2. Instantiate the transformer object\n", "3. Fit the transformer object (on training data only)\n", "4. Transform data using the transformer object\n", "5. Add the transformed data to the other data that was not transformed"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Replace None with appropriate code\n", "from sklearn.impute import MissingIndicator\n", "\n", "# (1) Identify data to be transformed\n", "# We only want missing indicators for LotFrontage\n", "frontage_train = X_train[[\"LotFrontage\"]]\n", "\n", "# (2) Instantiate the transformer object\n", "missing_indicator = MissingIndicator()\n", "\n", "# (3) Fit the transformer object on frontage_train\n", "None\n", "\n", "# (4) Transform frontage_train and assign the result\n", "# to frontage_missing_train\n", "frontage_missing_train = None\n", "\n", "# Visually inspect frontage_missing_train\n", "frontage_missing_train"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The result of transforming `frontage_train` should be an array of arrays, each containing `True` or `False`. Make sure the `assert`s pass before moving on to the next step."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "import numpy as np\n", "\n", "# frontage_missing_train should be a NumPy array\n", "assert type(frontage_missing_train) == np.ndarray\n", "\n", "# We should have the same number of rows as the full X_train\n", "assert frontage_missing_train.shape[0] == X_train.shape[0]\n", "\n", "# But we should only have 1 column\n", "assert frontage_missing_train.shape[1] == 1"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now let's add this new information as a new column of `X_train`:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "\n", "# (5) add the transformed data to the other data\n", "X_train[\"LotFrontage_Missing\"] = frontage_missing_train\n", "X_train"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "\n", "# Now we should have 1 extra column compared to\n", "# our original subset\n", "assert X_train.shape[1] == len(relevant_columns) + 1"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Imputing Missing Values for LotFrontage\n", "\n", "Now that we have noted where missing values were originally present, let's use a `SimpleImputer` ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html)) to fill in those NaNs in the `LotFrontage` column.\n", "\n", "The process is very similar to the `MissingIndicator` process, except that we want to replace the original `LotFrontage` column with the transformed version instead of just adding a new column on.\n", "\n", "In the cell below, create and use a `SimpleImputer` with `strategy=\"median\"` to transform the value of `frontage_train` (declared above)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Replace None with appropriate code\n", "\n", "from sklearn.impute import SimpleImputer\n", "\n", "# (1) frontage_train was created previously, so we don't\n", "# need to extract the relevant data again\n", "\n", "# (2) Instantiate a SimpleImputer with strategy=\"median\"\n", "imputer = None\n", "\n", "# (3) Fit the imputer on frontage_train\n", "None\n", "\n", "# (4) Transform frontage_train using the imputer and\n", "# assign the result to frontage_imputed_train\n", "frontage_imputed_train = None\n", "\n", "# Visually inspect frontage_imputed_train\n", "frontage_imputed_train"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now we can replace the original value of `LotFrontage` in `X_train` with the new value:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "\n", "# (5) Replace value of LotFrontage\n", "X_train[\"LotFrontage\"] = frontage_imputed_train\n", "\n", "# Visually inspect X_train\n", "X_train"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now the shape of `X_train` should still be the same as before:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "assert X_train.shape == (1095, 16)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["And now our only NaN values should be in `FireplaceQu`, which are NaN values but not missing values:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "X_train.isna().sum()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Great! Now we have completed Step 2."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python (learn-env)", "language": "python", "name": "learn-env"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.5"}}, "nbformat": 4, "nbformat_minor": 4}